import streamlit as st # Importamos Streamlit para crear la interfaz web
import speech_recognition as sr
import pyttsx3
from langchain_community.chat_message_histories import ChatMessageHistory # Esto es para manejar el historial de mensajes de chat
from langchain_core.prompts import PromptTemplate # Esto para formatear las preguntas o prompts que le pasamos al modelo
from langchain_ollama import OllamaLLM # Esto es para cargar el modelo de Ollama

# Cargar el modelo de Ollama
llm = OllamaLLM(model="mistral")
# Inicializar el historial de mensajes de chat si no existe en la sesiÃ³n de Streamlit
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = ChatMessageHistory()
    
# Inicializar el motor de texto a voz
engine = pyttsx3.init()
engine.setProperty('rate', 160)  # Ajustar la velocidad de habla
# Inicializar el reconocedor de voz
recognizer = sr.Recognizer()

# FunciÃ³n para escuchar y reconocer voz
def listen():
    with sr.Microphone() as source:
        st.write("ðŸŽ¤ Escuchando...")
        recognizer.adjust_for_ambient_noise(source) # Ajustar el ruido ambiente
        audio = recognizer.listen(source)
        try:
            text = recognizer.recognize_google(audio, language='es-ES')
            st.write(f"ðŸ”ˆ Has dicho: {text}")
            return text.lower()
        except sr.UnknownValueError:
            st.write("ðŸ¤– No he entendido lo que has dicho.")
            return None
        except sr.RequestError as e:
            st.write(f"âš   Error al conectar con el servicio de reconocimiento de voz: {e}")
            return None

# Definir una plantilla de prompt para formatear las preguntas
prompt_template = PromptTemplate(
    input_variables=["chat_history", "question"],
    template="ConversaciÃ³n previa: {chat_history}\nUsuario: {question}\nIA:"
)

# FunciÃ³n para procesar las respuestas de la IA
def run_chain(question):
    # Actualizar el historial de chat con la pregunta del usuario
    chat_history_text = "\n".join([
        f"{'Usuario' if msg.type == 'human' else 'IA'}: {msg.content}"
        for msg in st.session_state.chat_history.messages
    ])
    # Formatear el prompt con el historial de chat y la pregunta
    prompt = prompt_template.format(chat_history=chat_history_text, question=question)
    # Invocar el modelo LLM con el prompt formateado
    response = llm.invoke(prompt)
    # Agregar la pregunta del usuario y la respuesta de la IA al historial de chat
    st.session_state.chat_history.add_user_message(question)
    st.session_state.chat_history.add_ai_message(response)
    return response

# Streamlit UI
st.title("ðŸ¤– Asistente de Voz IA")
st.write("PregÃºntame cualquier cosa, y harÃ© todo lo posible para ayudarte.")
# BotÃ³n para escuchar la pregunta del usuario
if st.button("ðŸŽ¤ Escuchar Pregunta"):
    user_input = listen()
    if user_input:
        response = run_chain(user_input)
        st.write(f"ðŸ’¬ TÃº: {user_input}")
        st.write(f"ðŸ¤– IA: {response}")
        # Hablar la respuesta de la IA
        engine.say(response)
        engine.runAndWait()
# Mostrar el historial de chat
st.subheader("ðŸ”‚ Historial de Chat")
for msg in st.session_state.chat_history.messages:
    st.write(f"{'Usuario' if msg.type == 'human' else 'IA'}: {msg.content}")
# BotÃ³n para reiniciar el historial de chat
if st.button("ðŸ”„ Reiniciar Historial de Chat"):
    st.session_state.chat_history = ChatMessageHistory()
    st.write("Historial de chat reiniciado.")